{%- macro header() -%}
/* Launch configuration:
 *   work group size     : {{ launch.work_group_size }}
 *   partition size      : {{ launch.partition_size }}
 *   local memory size   : {{ launch.local_memory_size }}
 *   local memory offset : {{ launch.local_memory_offset }}
 *   warpsize            : {{ launch.warpsize }}
 */
#if defined(cl_khr_int64_base_atomics)
#pragma OPENCL EXTENSION cl_khr_int64_base_atomics : enable
#endif
#if defined(cl_khr_fp64)
#if defined(cl_amd_fp64)
#pragma OPENCL EXTENSION cl_amd_fp64 : enable
#else
#pragma OPENCL EXTENSION cl_khr_fp64 : enable
#endif
#elif defined(cl_amd_fp64)
#pragma OPENCL EXTENSION cl_amd_fp64 : enable
#endif

#define ROUND_UP(bytes) (((bytes) + 15) & ~15)
#define OP_WARPSIZE {{ launch.warpsize }}
#define OP2_STRIDE(arr, idx) (arr[idx])
{%- endmacro -%}

{%- macro stagingin(arg) -%}
  for (i_1 = get_local_id(0); i_1 < {{ shared_indirection_mapping_size_name(arg) }} * {{ arg._dat._dim[0] }}; i_1 += get_local_size(0)) {
  {{ shared_indirection_mapping_memory_name(arg) }}[i_1] = {{ dat_arg_name(arg) }}[i_1 % {{ arg._dat._dim[0] }} + {{ shared_indirection_mapping_name(arg) }}[i_1 / {{ arg._dat._dim[0] }}] * {{ arg._dat._dim[0] }}];
}
{%- endmacro -%}

{%- macro stagingout(arg) -%}
  for (i_1 = get_local_id(0); i_1 < {{ shared_indirection_mapping_size_name(arg) }} * {{ arg._dat._dim[0] }}; i_1 += get_local_size(0)) {
  {{ arg._dat._name }}[i_1 % {{ arg._dat._dim[0] }} + {{ shared_indirection_mapping_name(arg) }}[i_1 / {{ arg._dat._dim[0] }}] * {{ arg._dat._dim[0] }}] = {{ shared_indirection_mapping_memory_name(arg) }}[i_1];
}
{%- endmacro -%}

{%- macro mapping_array_name(it) -%}
  mapping_array_{{ it._dat._name }}_at_{{ it._idx }}_via_{{ it._map._name }}
{%- endmacro -%}

{%- macro global_reduc_local_name(it) -%}
  {{ it._dat._name }}_gbl_reduc_local
{%- endmacro -%}

{%- macro global_reduc_device_array_name(it) -%}
  {{ it._dat._name }}_gbl_reduc_device_array
{%- endmacro -%}

{%- macro dat_vec_name(it) -%}
  {{ it._dat._name }}_via_{{ it._map._name }}_vec
{%- endmacro -%}

{%- macro reduc_arg_local_name(it) -%}
  {{ it._dat._name }}_via_{{ it._map._name }}_at_{{ it._idx }}_local
{%- endmacro -%}

{%- macro dat_arg_name(it) -%}
  {{ it._dat._name }}
{%- endmacro -%}

{%- macro shared_indirection_mapping_name(it) -%}
  {{ it._dat._name }}_via_{{ it._map._name }}_indirection_map
{%- endmacro -%}

{%- macro shared_indirection_mapping_size_name(it) -%}
  {{ it._dat._name }}_via_{{ it._map._name }}_indirection_size
{%- endmacro -%}

{%- macro shared_indirection_mapping_memory_name(it) -%}
  {{ it._dat._name }}_via_{{ it._map._name }}_indirection
{%- endmacro -%}

{%- macro shared_indirection_mapping_idx_name(it) -%}
  {{ it._dat._name }}_via_{{ it._map._name }}_idx
{%- endmacro -%}

{%- macro shared_indirection_mapping_arg_name(it) -%}
  ind_{{ it._dat._name }}_via_{{ it._map._name }}_map
{%- endmacro -%}

{%- macro reduction_op(it) -%}
   {% if(it._is_INC) -%}
      reduction_tmp_array[lid] += reduction_tmp_array[lid + offset];
   {%- elif(it._is_MIN) -%}
      reduction_tmp_array[lid] = min(reduction_tmp_array[lid], reduction_tmp_array[lid + offset]);
   {%- elif(it._is_MAX) -%}
      reduction_tmp_array[lid] = max(reduction_tmp_array[lid], reduction_tmp_array[lid + offset]);
   {%- else -%}
      SOMETHING WENT SOUTH;
   {%- endif -%}
{%- endmacro -%}

{%- macro reduction_kernel(it) -%}
__kernel
void {{ it._dat._name }}_reduction_kernel (
  __global {{ it._dat._cl_type }}* reduction_result,
  __private {{ it._dat._cl_type }} input_value,
  __local {{ it._dat._cl_type }}* reduction_tmp_array
) {
  barrier(CLK_LOCAL_MEM_FENCE);
  int lid = get_local_id(0);
  reduction_tmp_array[lid] = input_value;
  barrier(CLK_LOCAL_MEM_FENCE);

  for(int offset = 1; offset < (int)get_local_size(0); offset <<= 1) {
    int mask = (offset << 1) - 1;
    if(((lid & mask) == 0) && (lid + offset < (int)get_local_size(0))) {
      {{ reduction_op(it) }}
    }
    barrier(CLK_LOCAL_MEM_FENCE);
  }

  if (lid == 0)
    *reduction_result = reduction_tmp_array[0];
}
{%- endmacro -%}

{%- macro populate_vec_map(it) -%}
// populate vec map
{%- if(it._is_indirect_reduction) -%}
{%- for it in it._i_gen_vec %}
  {{ dat_vec_name(it) }}[{{ it._idx }}] = {{ reduc_arg_local_name(it) }};
{% endfor -%}
{%- else -%}
{%- for it in it._i_gen_vec %}
  {{ dat_vec_name(it) }}[{{ it._idx }}] = &{{ shared_indirection_mapping_memory_name(it) }}[{{ mapping_array_name(it) }}[i_1 + shared_memory_offset] * {{ it._dat._dim[0] }}];
{%- endfor -%}
{%- endif -%}
{%- endmacro -%}

{%- macro staged_arg_local_variable_zeroing(it) -%}
for (i_2 = 0; i_2 < {{ it._dat._dim[0] }}; ++i_2) {
  {{ reduc_arg_local_name(it) }}[i_2] = {{ it._dat._cl_type_zero }};
}
{%- endmacro -%}

{%- macro reduction(it) -%}
for (i_2 = 0; i_2 < {{ it._dat._dim[0] }}; ++i_2) {
   {%- if(it._is_INC) %}
      {{ shared_indirection_mapping_memory_name(it) }}[i_2 + {{ mapping_array_name(it) }}[i_1 + shared_memory_offset] * {{ it._dat._dim[0] }}] += {{ reduc_arg_local_name(it) }}[i_2];
   {% elif(it._is_MIN) %}
      {{ shared_indirection_mapping_memory_name(it) }}[i_2 + {{ mapping_array_name(it) }}[i_1 + shared_memory_offset] * {{ it._dat._dim[0] }}] = min({{ shared_indirection_mapping_memory_name(it) }}[i_2 + {{ mapping_array_name(it) }}[i_1 + shared_memory_offset] * {{ it._dat._dim[0] }}], {{ reduc_arg_local_name(it) }}[i_2]);
   {% elif(it._is_MAX) %}
      {{ shared_indirection_mapping_memory_name(it) }}[i_2 + {{ mapping_array_name(it) }}[i_1 + shared_memory_offset] * {{ it._dat._dim[0] }}] = max({{ shared_indirection_mapping_memory_name(it) }}[i_2 + {{ mapping_array_name(it) }}[i_1 + shared_memory_offset] * {{ it._dat._dim[0] }}], {{ reduc_arg_local_name(it) }}[i_2]);
   {% else %}
      SOMETHING WENT SOUTH;
   {% endif %}
}
{%- endmacro -%}

{%- macro reduction2(it) -%}
  for (i_1 = get_local_id(0); i_1 < {{ shared_indirection_mapping_size_name(it) }} * {{ it._dat._dim[0] }}; i_1 += get_local_size(0)) {
  {{ it._dat._name }}[i_1 % {{ it._dat._dim[0] }} + {{ shared_indirection_mapping_name(it) }}[i_1 / {{ it._dat._dim[0] }}] * {{ it._dat._dim[0] }}] += {{ shared_indirection_mapping_memory_name(it) }}[i_1];
}
{%- endmacro -%}

{%- macro global_reduction_local_zeroing(it) -%}
for (i_1 = 0; i_1 < {{ it._dat._dim[0] }}; ++i_1) {
  {{ global_reduc_local_name(it) }}[i_1] = {{ it._dat._cl_type_zero }};
}
{%- endmacro -%}

{%- macro on_device_global_reduction(it) -%}
// THIS TEMPLATE SHOULD BE FACTORISED WITH DIRECT LOOPS REDUCTIONS
for (i_1 = 0; i_1 < {{ it._dat._dim[0] }}; ++i_1)
{
  {{ it._dat._name }}_reduction_kernel(&{{ global_reduc_device_array_name(it) }}[i_1 + get_group_id(0) * {{ it._dat._dim[0] }}], {{ global_reduc_local_name(it) }}[i_1], (__local {{ it._dat._cl_type }}*) shared);
}
{%- endmacro -%}

{%- macro kernel_stub() -%}
__kernel
__attribute__((reqd_work_group_size({{ launch.work_group_size }}, 1, 1)))
void {{ parloop._kernel._name }}_stub(
  {%- for it in parloop._unique_dats %}
  __global {{ it._cl_type }}* {{ it._name }},
  {%- endfor -%}
  {% for it in parloop._global_non_reduction_args %}
  __global {{ it._dat._cl_type }}* {{ it._dat._name }},
  {%- endfor -%}
  {% for it in parloop._global_reduction_args %}
  __global {{ it._dat._cl_type }}* {{ global_reduc_device_array_name(it) }},
  {%- endfor -%}
  {% for it in op2const %}
  __constant {{ it._cl_type }}* {{ it._name }},
  {% endfor %}
  {% for it in parloop._dat_map_pairs %}
  __global int* {{ shared_indirection_mapping_arg_name(it) }},
  {%- endfor -%}
  {% for it in parloop._args %}
  {% if(it._is_indirect) %}__global short* {{ mapping_array_name(it) }},{% endif %}
  {%- endfor -%}
  {% for it in parloop._unique_matrix %}
  __global {{ it._cl_type }}* {{ it._name }},
  __global int* {{ it._name }}_rowptr,
  __global int* {{ it._name }}_colidx,
  {%- endfor -%}
  {% for it in parloop._matrix_entry_maps %}
  __global int* {{ it._name }},
  {%- endfor -%}

  __global int* p_ind_sizes,
  __global int* p_ind_offsets,
  __global int* p_blk_map,
  __global int* p_offset,
  __global int* p_nelems,
  __global int* p_nthrcol,
  __global int* p_thrcol,
  __private int block_offset
) {
  __local char shared [{{ launch.local_memory_size }}] __attribute__((aligned(sizeof(long))));
  __local int shared_memory_offset;
  __local int active_threads_count;

  int nbytes;
  int block_id;

  int i_1;

{%- if(parloop._indirect_reduc_args) %}
  __local int colors_count;
  __local int active_threads_count_ceiling;
  int color_1;
  int color_2;
  int i_2;

  // reduction args
{%- for it in parloop._indirect_reduc_args %}
  {{ it._dat._cl_type }} {{ reduc_arg_local_name(it) }}[{{ it._dat._dim[0] }}];
{%- endfor %}
{%- endif %}

{%- if(parloop._global_reduction_args) %}
  // global reduction local declarations
{% for it in parloop._global_reduction_args %}
  {{ it._dat._cl_type }} {{ global_reduc_local_name(it) }}[{{ it._dat._dim[0] }}];
{%- endfor %}
{%- endif %}

{% if(parloop._matrix_args) %}
  // local matrix entry
  {% for it in parloop._matrix_args %}
  __private {{ it._dat._cl_type }} {{ it._dat._name }}_entry;
  {% endfor %}
{% endif %}

  // shared indirection mappings
{%- for it in parloop._dat_map_pairs %}
  __global int* __local {{ shared_indirection_mapping_name(it) }};
{%- endfor -%}
{% for it in parloop._dat_map_pairs %}
  __local int {{ shared_indirection_mapping_size_name(it) }};
{%- endfor -%}
{% for it in parloop._dat_map_pairs %}
  __local {{ it._dat._cl_type }}* __local {{ shared_indirection_mapping_memory_name(it) }};
{%- endfor -%}
{% for it in parloop._dat_map_pairs %}
  const int {{ shared_indirection_mapping_idx_name(it) }} = {{ loop.index0 }};
{%- endfor %}
{% for it in parloop._nonreduc_vec_dat_map_pairs %}
  __local {{ it._dat._cl_type }}* {{ dat_vec_name(it) }}[{{ it._map._dim }}];
{%- endfor %}
{% for it in parloop._reduc_vec_dat_map_pairs %}
  {{ it._dat._cl_type }}* {{ dat_vec_name(it) }}[{{ it._map._dim }}];
{%- endfor %}

  if (get_local_id(0) == 0) {
    block_id = p_blk_map[get_group_id(0) + block_offset];
    active_threads_count = p_nelems[block_id];
{%- if(parloop._indirect_reduc_args) %}
    active_threads_count_ceiling = get_local_size(0) * (1 + (active_threads_count - 1) / get_local_size(0));
    colors_count = p_nthrcol[block_id];
{%- endif %}
    shared_memory_offset = p_offset[block_id];
{% for it in parloop._dat_map_pairs %}
    {{ shared_indirection_mapping_size_name(it) }} = p_ind_sizes[{{ shared_indirection_mapping_idx_name(it) }} + block_id * {{ launch.ninds }}];
{%- endfor %}

{%- for it in parloop._dat_map_pairs %}
    {{ shared_indirection_mapping_name(it) }} = {{ shared_indirection_mapping_arg_name(it) }} + p_ind_offsets[{{ shared_indirection_mapping_idx_name(it) }} + block_id * {{ launch.ninds }}];
{%- endfor %}

    nbytes = 0;
{%- for it in parloop._dat_map_pairs %}
    {{ shared_indirection_mapping_memory_name(it) }} = (__local {{ it._dat._cl_type }}*) (&shared[nbytes]);
    nbytes += ROUND_UP({{ shared_indirection_mapping_size_name(it) }} * {{ it._dat._dim[0] }} * sizeof({{ it._dat._cl_type }}));
{%- endfor %}
  }
  barrier(CLK_LOCAL_MEM_FENCE);

{% if(parloop._read_dat_map_pairs) -%}
  // staging in of indirect dats
  {% for it in parloop._read_dat_map_pairs %}
  {{ stagingin(it) }}
  {% endfor %}
  barrier(CLK_LOCAL_MEM_FENCE);
{% endif %}

{%- if(parloop._indirect_reduc_dat_map_pairs) %}
  // zeroing local memory for indirect reduction
  {% for it in parloop._indirect_reduc_dat_map_pairs %}
  {{ shared_memory_reduc_zeroing(it) | indent(2) }}
  {% endfor %}
  barrier(CLK_LOCAL_MEM_FENCE);
{% endif %}

{%- if(parloop._global_reduction_args) %}
  // zeroing private memory for global reduction
  {% for it in parloop._global_reduction_args %}
  {{ global_reduction_local_zeroing(it) }}
  {% endfor %}
{% endif %}

{%- if(parloop._indirect_reduc_args) %}
  for (i_1 = get_local_id(0); i_1 < active_threads_count_ceiling; i_1 += get_local_size(0)) {
    color_2 = -1;
    if (i_1 < active_threads_count) {
      {% for it in parloop._indirect_reduc_args %}
      {{ staged_arg_local_variable_zeroing(it) | indent(6) }}
      {%- endfor %}

      {{ kernel_call() | indent(6) }}
      color_2 = p_thrcol[i_1 + shared_memory_offset];
    }
    for (color_1 = 0; color_1 < colors_count; ++color_1) {
      // should there be a if + barrier pattern for each indirect reduction argument ?
      if (color_2 == color_1) {
        {% for it in parloop._indirect_reduc_args %}
        {{ reduction(it) | indent(8) }}
    {% endfor %}
      }
      barrier(CLK_LOCAL_MEM_FENCE);
    }
  }
{%- else %}
  for (i_1 = get_local_id(0); i_1 < active_threads_count; i_1 += get_local_size(0)) {
    {{ kernel_call() | indent(6) }}
  }
{%- endif %}

{%- if(parloop._indirect_reduc_dat_map_pairs) %}
  {% for it in parloop._indirect_reduc_dat_map_pairs %}
  {{ reduction2(it) | indent(2) }}
  {%- endfor %}
{%- endif %}

{%- if(parloop._written_dat_map_pairs) %}
  // staging out indirect dats
  barrier(CLK_LOCAL_MEM_FENCE);
  {% for it in parloop._written_dat_map_pairs %}
  {{ stagingout(it) | indent(2) }}
  {%- endfor %}
{%- endif %}

{%- if(parloop._global_reduction_args) %}
  barrier(CLK_LOCAL_MEM_FENCE);
  // on device global reductions
  {% for it in parloop._global_reduction_args %}
  {{ on_device_global_reduction(it) | indent(2) }}
  {%- endfor %}
{%- endif %}
}
{%- endmacro -%}

{#- rewrite: do recursive template -#}
{%- macro matrix_kernel_call() -%}
// IterationSpace index loops ({{ parloop._it_space._extent_ranges }})
{%- for it in parloop._it_space._extent_ranges %}
for (int idx_{{ loop.index0 }} = 0; idx_{{ loop.index0 }} < {{ it }}; ++idx_{{ loop.index0 }}) {
{%- endfor %}
{% for it in parloop._matrix_args %}
{{ it._dat._name }}_entry = {{ it._dat._cl_type_zero }};
{% endfor %}
{{ parloop._kernel._name }}(
  {% filter trim|replace("\n", ",\n") -%}
  {%- for it in parloop._actual_args %}
  {{ kernel_call_arg(it) }}
  {%- endfor -%}
  {{- kernel_call_const_args() -}}
  {%- for it in parloop._it_space._extent_ranges %}
  idx_{{ loop.index0 }}
  {%- endfor -%}
  {%- endfilter %}
  );

{% for arg in parloop._matrix_args -%}
{%- if(arg._is_INC) -%}
  matrix_add
{%- else -%}
  matrix_set
{%- endif -%}(
  {{ arg._dat._name }},
  {{ arg._dat._name }}_rowptr,
  {{ arg._dat._name }}_colidx,
  {%- for map in arg._map %}
  {% set ext = parloop._it_space._extent_ranges[loop.index0] -%}
  {{ map._name }}[(i_1 + shared_memory_offset) * {{ ext }} + idx_{{ loop.index0 }}],
  {%- endfor %}
  {{ arg._dat._name }}_entry
);
{% endfor %}
{%- for it in parloop._it_space._extent_ranges %}
}
{%- endfor -%}
{%- endmacro -%}

{%- macro kernel_call() -%}
{% for it in parloop._actual_args if(it._is_vec_map) %}
  {{ populate_vec_map(it) }}
{% endfor %}
{% if(parloop._it_space) %}
{{ matrix_kernel_call() }}
{% else %}
{{ parloop._kernel._name }}(
  {% filter trim|replace("\n", ",\n") -%}
  {%- for it in parloop._actual_args -%}
  {{ kernel_call_arg(it) }}
  {% endfor -%}
  {{ kernel_call_const_args() }}
  {%- endfilter %}
);
{% endif %}
{%- endmacro -%}

{%- macro kernel_call_const_args() -%}
{%- for c in op2const -%}
{% if(c._is_scalar) %}*{% endif %}{{ c._name }}
{% endfor -%}
{%- endmacro -%}

{%- macro kernel_call_arg(it) -%}
{% if(it._is_direct) -%}
  {{ typecast("__global", it._dat._cl_type + "*", "__private") -}}
  ({{ it._dat._name }} + (i_1 + shared_memory_offset) * {{ it._dat._dim[0] }})
{%- elif(it._is_mat) -%}
  &{{ it._dat._name }}_entry
{%- elif(it._is_vec_map) -%}
  {{ dat_vec_name(it) }}
{%- elif(it._is_global_reduction) -%}
  {{ global_reduc_local_name(it) }}
{%- elif(it._is_indirect_reduction) -%}
  {{ reduc_arg_local_name(it) }}
{%- elif(it._is_global) -%}
  {{ it._dat._name }}
{%- else -%}
  &{{ shared_indirection_mapping_memory_name(it) }}[{{ mapping_array_name(it) }}[i_1 + shared_memory_offset] * {{ it._dat._dim[0] }}]
{%- endif -%}
{%- endmacro -%}

{%- macro typecast(storage, type, qualifier) -%}
({{ storage }} {{ type }}{% if(not codegen.amd) %} {{ qualifier }}{% endif %})
{%- endmacro -%}

{%- macro shared_memory_reduc_zeroing(it) -%}
for (i_1 = get_local_id(0); i_1 < {{ shared_indirection_mapping_size_name(it) }} * {{ it._dat._dim[0] }}; i_1 += get_local_size(0)) {
  {{ shared_indirection_mapping_memory_name(it) }}[i_1] = 0;
}
{%- endmacro -%}

{%- macro matrix_support() -%}
void matrix_atomic_add(__global double* dst, double value);
void matrix_atomic_add(__global double* dst, double value)
{
#if defined(cl_khr_int64_base_atomics)
  {{ union_decl() }}
  do
  {
    old.val = *dst;
    new.val = old.val + value;
  } while (atom_cmpxchg((volatile __global unsigned long int*) dst, old.dummy, new.dummy) != old.dummy);
#else
  *dst = *dst + value;
#endif
}

void matrix_atomic_set(__global double* dst, double value);
void matrix_atomic_set(__global double* dst, double value)
{
#if defined(cl_khr_int64_base_atomics)
  {{ union_decl() }}
  do
  {
    old.val = 0.0;
    new.val = value;
  } while (atom_cmpxchg((volatile __global unsigned long int*) dst, old.dummy, new.dummy) != old.dummy);
#else
  *dst = value;
#endif
}

int rc2offset(__global int* mat_rowptr, __global int* mat_colidx, int r, int c);
int rc2offset(__global int* mat_rowptr, __global int* mat_colidx, int r, int c)
{
  int offset = mat_rowptr[r];
  int end = mat_rowptr[r+1];
  __global int * cursor;
  for (cursor = &mat_colidx[offset]; cursor < &mat_colidx[end]; ++cursor)
  {
    if (*cursor == c) break;
    ++offset;
  }
  return offset;
}

void matrix_add(__global double* mat_array, __global int* mat_rowptr, __global int* mat_colidx, int r, int c, double v);
void matrix_add(__global double* mat_array, __global int* mat_rowptr, __global int* mat_colidx, int r, int c, double v)
{
  int offset = rc2offset(mat_rowptr, mat_colidx, r, c);
  matrix_atomic_add(mat_array + offset, v);
}

void matrix_set(__global double* mat_array, __global int* mat_rowptr, __global int* mat_colidx, int r, int c, double v);
void matrix_set(__global double* mat_array, __global int* mat_rowptr, __global int* mat_colidx, int r, int c, double v)
{
  int offset = rc2offset(mat_rowptr, mat_colidx, r, c);
  matrix_atomic_set(mat_array + offset, v);
}
{%- endmacro -%}

{%- macro union_decl() -%}
  union {
    unsigned long dummy;
    double val;
  } new;

  union {
    unsigned long dummy;
    double val;
  } old;
{%- endmacro -%}



{{- header() }}

{% for it in parloop._global_reduction_args -%}
  {{ reduction_kernel(it) }}
{% endfor %}
{{ user_kernel }}
{{ matrix_support() }}

{{ kernel_stub() }}
{%- macro header() -%}
/* Launch configuration:
 *   work group size     : {{ launch.work_group_size }}
 *   partition size      : {{ launch.partition_size }}
 *   local memory size   : {{ launch.local_memory_size }}
 *   local memory offset : {{ launch.local_memory_offset }}
 *   warpsize            : {{ launch.warpsize }}
 */
{% if(parloop._matrix_args) %}
#if defined(cl_khr_int64_base_atomics)
#pragma OPENCL EXTENSION cl_khr_int64_base_atomics : enable
#endif
{% endif %}
#if defined(cl_khr_fp64)
#if defined(cl_amd_fp64)
#pragma OPENCL EXTENSION cl_amd_fp64 : enable
#else
#pragma OPENCL EXTENSION cl_khr_fp64 : enable
#endif
#elif defined(cl_amd_fp64)
#pragma OPENCL EXTENSION cl_amd_fp64 : enable
#endif

#define ROUND_UP(bytes) (((bytes) + 15) & ~15)
#define OP_WARPSIZE {{ launch.warpsize }}
#define OP2_STRIDE(arr, idx) (arr[idx])
{%- endmacro -%}

{%- macro stagingin(arg) -%}
  for (i_1 = get_local_id(0); i_1 < {{ shared_indirection_mapping_size_name(arg) }} * {{ arg.data.cdim }}; i_1 += get_local_size(0)) {
  {{ shared_indirection_mapping_memory_name(arg) }}[i_1] = {{ dat_arg_name(arg) }}[i_1 % {{ arg.data.cdim }} + {{ shared_indirection_mapping_name(arg) }}[i_1 / {{ arg.data.cdim }}] * {{ arg.data.cdim }}];
}
{%- endmacro -%}

{%- macro stagingout(arg) -%}
  for (i_1 = get_local_id(0); i_1 < {{ shared_indirection_mapping_size_name(arg) }} * {{ arg.data.cdim }}; i_1 += get_local_size(0)) {
  {{ arg.data.name }}[i_1 % {{ arg.data.cdim }} + {{ shared_indirection_mapping_name(arg) }}[i_1 / {{ arg.data.cdim }}] * {{ arg.data.cdim }}] = {{ shared_indirection_mapping_memory_name(arg) }}[i_1];
}
{%- endmacro -%}

{%- macro mapping_array_name(arg) -%}
  mapping_array_{{ arg.data.name }}_at_{{ arg.idx }}_via_{{ arg.map.name }}
{%- endmacro -%}

{%- macro global_reduc_local_name(arg) -%}
  {{ arg.data.name }}_gbl_reduc_local
{%- endmacro -%}

{%- macro global_reduc_device_array_name(arg) -%}
  {{ arg.data.name }}_gbl_reduc_device_array
{%- endmacro -%}

{%- macro dat_vec_name(arg) -%}
  {{ arg.data.name }}_via_{{ arg.map.name }}_vec
{%- endmacro -%}

{%- macro reduc_arg_local_name(arg) -%}
  {{ arg.data.name }}_via_{{ arg.map.name }}_at_{{ arg.idx }}_local
{%- endmacro -%}

{%- macro dat_arg_name(arg) -%}
  {{ arg.data.name }}
{%- endmacro -%}

{%- macro shared_indirection_mapping_name(arg) -%}
  {{ arg.data.name }}_via_{{ arg.map.name }}_indirection_map
{%- endmacro -%}

{%- macro shared_indirection_mapping_size_name(arg) -%}
  {{ arg.data.name }}_via_{{ arg.map.name }}_indirection_size
{%- endmacro -%}

{%- macro shared_indirection_mapping_memory_name(arg) -%}
  {{ arg.data.name }}_via_{{ arg.map.name }}_indirection
{%- endmacro -%}

{%- macro shared_indirection_mapping_idx_name(arg) -%}
  {{ arg.data.name }}_via_{{ arg.map.name }}_idx
{%- endmacro -%}

{%- macro shared_indirection_mapping_arg_name(arg) -%}
  ind_{{ arg.data.name }}_via_{{ arg.map.name }}_map
{%- endmacro -%}

{%- macro reduction_op(arg) -%}
   {% if(arg._is_INC) -%}
      reduction_tmp_array[lid] += reduction_tmp_array[lid + offset];
   {%- elif(arg._is_MIN) -%}
      reduction_tmp_array[lid] = min(reduction_tmp_array[lid], reduction_tmp_array[lid + offset]);
   {%- elif(arg._is_MAX) -%}
      reduction_tmp_array[lid] = max(reduction_tmp_array[lid], reduction_tmp_array[lid + offset]);
   {%- endif -%}
{%- endmacro -%}

{%- macro reduction_kernel(arg) -%}
__kernel
void {{ arg.data.name }}_reduction_kernel (
  __global {{ arg.data._cl_type }}* reduction_result,
  __private {{ arg.data._cl_type }} input_value,
  __local {{ arg.data._cl_type }}* reduction_tmp_array
) {
  barrier(CLK_LOCAL_MEM_FENCE);
  int lid = get_local_id(0);
  reduction_tmp_array[lid] = input_value;
  barrier(CLK_LOCAL_MEM_FENCE);

  for(int offset = 1; offset < (int)get_local_size(0); offset <<= 1) {
    int mask = (offset << 1) - 1;
    if(((lid & mask) == 0) && (lid + offset < (int)get_local_size(0))) {
      {{ reduction_op(arg) }}
    }
    barrier(CLK_LOCAL_MEM_FENCE);
  }

  if (lid == 0)
    *reduction_result = reduction_tmp_array[0];
}
{%- endmacro -%}

{%- macro populate_vec_map(arg) -%}
// populate vec map
{%- if(arg._is_indirect_reduction) -%}
{%- for arg in arg._i_gen_vec %}
  {{ dat_vec_name(arg) }}[{{ arg.idx }}] = {{ reduc_arg_local_name(arg) }};
{% endfor -%}
{%- else -%}
{%- for arg in arg._i_gen_vec %}
  {{ dat_vec_name(arg) }}[{{ arg.idx }}] = &{{ shared_indirection_mapping_memory_name(arg) }}[{{ mapping_array_name(arg) }}[i_1 + shared_memory_offset] * {{ arg.data.cdim }}];
{%- endfor -%}
{%- endif -%}
{%- endmacro -%}

{%- macro staged_arg_local_variable_zeroing(arg) -%}
for (i_2 = 0; i_2 < {{ arg.data.cdim }}; ++i_2) {
  {{ reduc_arg_local_name(arg) }}[i_2] = {{ arg.data._cl_type_zero }};
}
{%- endmacro -%}

{%- macro color_reduction(arg) -%}
for (i_2 = 0; i_2 < {{ arg.data.cdim }}; ++i_2) {
   {%- if(arg._is_INC) %}
      {{ shared_indirection_mapping_memory_name(arg) }}[i_2 + {{ mapping_array_name(arg) }}[i_1 + shared_memory_offset] * {{ arg.data.cdim }}] += {{ reduc_arg_local_name(arg) }}[i_2];
   {% elif(arg._is_MIN) %}
      {{ shared_indirection_mapping_memory_name(arg) }}[i_2 + {{ mapping_array_name(arg) }}[i_1 + shared_memory_offset] * {{ arg.data.cdim }}] = min({{ shared_indirection_mapping_memory_name(arg) }}[i_2 + {{ mapping_array_name(arg) }}[i_1 + shared_memory_offset] * {{ arg.data.cdim }}], {{ reduc_arg_local_name(arg) }}[i_2]);
   {% elif(arg._is_MAX) %}
      {{ shared_indirection_mapping_memory_name(arg) }}[i_2 + {{ mapping_array_name(arg) }}[i_1 + shared_memory_offset] * {{ arg.data.cdim }}] = max({{ shared_indirection_mapping_memory_name(arg) }}[i_2 + {{ mapping_array_name(arg) }}[i_1 + shared_memory_offset] * {{ arg.data.cdim }}], {{ reduc_arg_local_name(arg) }}[i_2]);
   {% endif %}
}
{%- endmacro -%}

{%- macro work_group_reduction(arg) -%}
  for (i_1 = get_local_id(0); i_1 < {{ shared_indirection_mapping_size_name(arg) }} * {{ arg.data.cdim }}; i_1 += get_local_size(0)) {
  {{ arg.data.name }}[i_1 % {{ arg.data.cdim }} + {{ shared_indirection_mapping_name(arg) }}[i_1 / {{ arg.data.cdim }}] * {{ arg.data.cdim }}] += {{ shared_indirection_mapping_memory_name(arg) }}[i_1];
}
{%- endmacro -%}

{%- macro reduction_id_value(arg) -%}
{%- if(arg._is_INC) -%}
{{ arg.data._cl_type_zero }}
{%- elif(arg._is_MIN) -%}
{{ arg.data._cl_type_max }}
{%- elif(arg._is_MAX) -%}
{{ arg.data._cl_type_min }}
{%- endif -%}
{%- endmacro -%}

{%- macro global_reduction_local_zeroing(arg) -%}
for (i_1 = 0; i_1 < {{ arg.data.cdim }}; ++i_1) {
  {{ global_reduc_local_name(arg) }}[i_1] = {{ reduction_id_value(arg) }};
}
{%- endmacro -%}

{%- macro on_device_global_reduction(arg) -%}
for (i_1 = 0; i_1 < {{ arg.data.cdim }}; ++i_1)
{
  {{ arg.data.name }}_reduction_kernel(&{{ global_reduc_device_array_name(arg) }}[i_1 + get_group_id(0) * {{ arg.data.cdim }}], {{ global_reduc_local_name(arg) }}[i_1], (__local {{ arg.data._cl_type }}*) shared);
}
{%- endmacro -%}

{%- macro kernel_stub() -%}
__kernel
__attribute__((reqd_work_group_size({{ launch.work_group_size }}, 1, 1)))
void {{ parloop._kernel.name }}_stub(
  {%- for arg in parloop._unique_dats %}
  __global {{ arg._cl_type }}* {{ arg.name }},
  {%- endfor -%}
  {% for arg in parloop._global_non_reduction_args %}
  __global {{ arg.data._cl_type }}* {{ arg.data.name }},
  {%- endfor -%}
  {% for arg in parloop._global_reduction_args %}
  __global {{ arg.data._cl_type }}* {{ global_reduc_device_array_name(arg) }},
  {%- endfor -%}
  {% for c in op2const %}
  __constant {{ c._cl_type }}* {{ c.name }},
  {% endfor %}
  {% for dm in parloop._dat_map_pairs %}
  __global int* {{ shared_indirection_mapping_arg_name(dm) }},
  {%- endfor -%}
  {% for arg in parloop._args %}
  {% if(arg._is_indirect) %}__global short* {{ mapping_array_name(arg) }},{% endif %}
  {%- endfor -%}
  {% for mat in parloop._unique_matrix %}
  __global {{ mat._cl_type }}* {{ mat.name }},
  __global int* {{ mat.name }}_rowptr,
  __global int* {{ mat.name }}_colidx,
  {%- endfor -%}
  {% for matem in parloop._matrix_entry_maps %}
  __global int* {{ matem.name }},
  {%- endfor -%}

  __global int* p_ind_sizes,
  __global int* p_ind_offsets,
  __global int* p_blk_map,
  __global int* p_offset,
  __global int* p_nelems,
  __global int* p_nthrcol,
  __global int* p_thrcol,
  __private int block_offset
) {
  __local char shared [{{ launch.local_memory_size }}] __attribute__((aligned(sizeof(long))));
  __local int shared_memory_offset;
  __local int active_threads_count;

  int nbytes;
  int block_id;

  int i_1;

{%- if(parloop._indirect_reduc_args) %}
  __local int colors_count;
  __local int active_threads_count_ceiling;
  int color_1;
  int color_2;
  int i_2;

  // reduction args
{%- for arg in parloop._indirect_reduc_args %}
  {{ arg.data._cl_type }} {{ reduc_arg_local_name(arg) }}[{{ arg.data.cdim }}];
{%- endfor %}
{%- endif %}

{%- if(parloop._global_reduction_args) %}
  // global reduction local declarations
{% for arg in parloop._global_reduction_args %}
  {{ arg.data._cl_type }} {{ global_reduc_local_name(arg) }}[{{ arg.data.cdim }}];
{%- endfor %}
{%- endif %}

{% if(parloop._matrix_args) %}
  // local matrix entry
  {% for arg in parloop._matrix_args %}
  __private {{ arg.data._cl_type }} {{ arg.data.name }}_entry
  {%- for dim in arg.data.sparsity.dims %}[{{ dim }}]{% endfor %};
  {% endfor %}
{% endif %}

  // shared indirection mappings
{%- for dm in parloop._dat_map_pairs %}
  __global int* __local {{ shared_indirection_mapping_name(dm) }};
  __local int {{ shared_indirection_mapping_size_name(dm) }};
  __local {{ dm.data._cl_type }}* __local {{ shared_indirection_mapping_memory_name(dm) }};
  const int {{ shared_indirection_mapping_idx_name(dm) }} = {{ loop.index0 }};
{%- endfor %}
{% for dm in parloop._nonreduc_vec_dat_map_pairs %}
  __local {{ dm.data._cl_type }}* {{ dat_vec_name(dm) }}[{{ dm.map.dim }}];
{%- endfor %}
{% for dm in parloop._reduc_vec_dat_map_pairs %}
  {{ dm.data._cl_type }}* {{ dat_vec_name(dm) }}[{{ dm.map.dim }}];
{%- endfor %}
{% for dm in parloop._nonreduc_itspace_dat_map_pairs %}
  __local {{ dm.data._cl_type }}* {{ dat_vec_name(dm) }}[{{ dm.map.dim }}];
{%- endfor %}
{% for dm in parloop._reduc_itspace_dat_map_pairs %}
  {{ dm.data._cl_type }}* {{ dat_vec_name(dm) }}[{{ dm.map.dim }}];
{%- endfor %}

  if (get_local_id(0) == 0) {
    block_id = p_blk_map[get_group_id(0) + block_offset];
    active_threads_count = p_nelems[block_id];
{%- if(parloop._indirect_reduc_args) %}
    active_threads_count_ceiling = get_local_size(0) * (1 + (active_threads_count - 1) / get_local_size(0));
    colors_count = p_nthrcol[block_id];
{%- endif %}
    shared_memory_offset = p_offset[block_id];
{% for dm in parloop._dat_map_pairs %}
    {{ shared_indirection_mapping_size_name(dm) }} = p_ind_sizes[{{ shared_indirection_mapping_idx_name(dm) }} + block_id * {{ launch.ninds }}];
    {{ shared_indirection_mapping_name(dm) }} = {{ shared_indirection_mapping_arg_name(dm) }} + p_ind_offsets[{{ shared_indirection_mapping_idx_name(dm) }} + block_id * {{ launch.ninds }}];
{%- endfor %}

    nbytes = 0;
{%- for dm in parloop._dat_map_pairs %}
    {{ shared_indirection_mapping_memory_name(dm) }} = (__local {{ dm.data._cl_type }}*) (&shared[nbytes]);
    nbytes += ROUND_UP({{ shared_indirection_mapping_size_name(dm) }} * {{ dm.data.cdim }} * sizeof({{ dm.data._cl_type }}));
{%- endfor %}
  }
  barrier(CLK_LOCAL_MEM_FENCE);

{% if(parloop._read_dat_map_pairs) -%}
  // staging in of indirect dats
  {% for dm in parloop._read_dat_map_pairs %}
  {{ stagingin(dm) }}
  {% endfor %}
  barrier(CLK_LOCAL_MEM_FENCE);
{% endif %}

{%- if(parloop._indirect_reduc_dat_map_pairs) %}
  // zeroing local memory for indirect reduction
  {% for dm in parloop._indirect_reduc_dat_map_pairs %}
  {{ shared_memory_reduc_zeroing(dm) | indent(2) }}
  {% endfor %}
  barrier(CLK_LOCAL_MEM_FENCE);
{% endif %}

{%- if(parloop._global_reduction_args) %}
  // zeroing private memory for global reduction
  {% for arg in parloop._global_reduction_args %}
  {{ global_reduction_local_zeroing(arg) }}
  {% endfor %}
{% endif %}

{%- if(parloop._indirect_reduc_args) %}
  for (i_1 = get_local_id(0); i_1 < active_threads_count_ceiling; i_1 += get_local_size(0)) {
    color_2 = -1;
    if (i_1 < active_threads_count) {
      {% for arg in parloop._indirect_reduc_args %}
      {{ staged_arg_local_variable_zeroing(arg) | indent(6) }}
      {%- endfor %}

      {{ kernel_call() | indent(6) }}
      color_2 = p_thrcol[i_1 + shared_memory_offset];
    }
    for (color_1 = 0; color_1 < colors_count; ++color_1) {
      // should there be a if + barrier pattern for each indirect reduction argument ?
      if (color_2 == color_1) {
        {% for arg in parloop._indirect_reduc_args %}
        {{ color_reduction(arg) | indent(8) }}
    {% endfor %}
      }
      barrier(CLK_LOCAL_MEM_FENCE);
    }
  }
{%- else %}
  for (i_1 = get_local_id(0); i_1 < active_threads_count; i_1 += get_local_size(0)) {
    {{ kernel_call() | indent(6) }}
  }
{%- endif %}

{%- if(parloop._indirect_reduc_dat_map_pairs) %}
  {% for dm in parloop._indirect_reduc_dat_map_pairs %}
  {{ work_group_reduction(dm) | indent(2) }}
  {%- endfor %}
{%- endif %}

{%- if(parloop._written_dat_map_pairs) %}
  // staging out indirect dats
  barrier(CLK_LOCAL_MEM_FENCE);
  {% for dm in parloop._written_dat_map_pairs %}
  {{ stagingout(dm) | indent(2) }}
  {%- endfor %}
{%- endif %}

{%- if(parloop._global_reduction_args) %}
  barrier(CLK_LOCAL_MEM_FENCE);
  // on device global reductions
  {% for arg in parloop._global_reduction_args %}
  {{ on_device_global_reduction(arg) | indent(2) }}
  {%- endfor %}
{%- endif %}
}
{%- endmacro -%}

{#- rewrite: do recursive template -#}
{%- macro matrix_kernel_call() -%}
// IterationSpace index loops ({{ parloop._it_space._extent_ranges }})
{%- for it in parloop._it_space._extent_ranges %}
for (int idx_{{ loop.index0 }} = 0; idx_{{ loop.index0 }} < {{ it }}; ++idx_{{ loop.index0 }}) {
{%- endfor %}
{% for arg in parloop._matrix_args %}
{% for dim in arg.data.sparsity.dims %}
for (int i{{ loop.index0 }}=0; i{{ loop.index0 }}<{{ dim }}; ++i{{ loop.index0 }})
{%- endfor %}
  {{ arg.data.name }}_entry[i0][i1] = {{ arg.data._cl_type_zero }};
{% endfor %}
{{ parloop._kernel.name }}(
  {% filter trim|replace("\n", ",\n") -%}
  {%- for arg in parloop.args %}
  {{ kernel_call_arg(arg) }}
  {%- endfor -%}
  {{- kernel_call_const_args() -}}
  {%- for ext in parloop._it_space._extent_ranges %}
  idx_{{ loop.index0 }}
  {%- endfor -%}
  {%- endfilter %}
  );

{% for arg in parloop._matrix_args -%}
{% for dim in arg.data.sparsity.dims %}
for (int i{{ loop.index0 }}=0; i{{ loop.index0 }}<{{ dim }}; ++i{{ loop.index0 }})
{%- endfor %}
  {% if(arg._is_INC) -%}
    matrix_add
  {%- else -%}
    matrix_set
  {%- endif -%}(
    {{ arg.data.name }},
    {{ arg.data.name }}_rowptr,
    {{ arg.data.name }}_colidx,
    {%- for map in arg._map %}
    {% set ext = parloop._it_space._extent_ranges[loop.index0] -%}
    {% set dim = arg.data.sparsity.dims[loop.index0] -%}
    {{ dim }}*{{ map.name }}[(i_1 + shared_memory_offset) * {{ ext }} + idx_{{ loop.index0 }}]+i{{ loop.index0 }},
    {%- endfor %}
    {{ arg.data.name }}_entry[i0][i1]
  );
{% endfor %}
{%- for it in parloop._it_space._extent_ranges %}
}
{%- endfor -%}
{%- endmacro -%}

{%- macro kernel_call() -%}
{% for arg in parloop.args if(arg._is_vec_map or arg._uses_itspace) %}
  {{ populate_vec_map(arg) }}
{% endfor %}
{% if(parloop._has_itspace) %}
{{ matrix_kernel_call() }}
{% else %}
{{ parloop._kernel.name }}(
  {% filter trim|replace("\n", ",\n") -%}
  {%- for arg in parloop.args -%}
  {{ kernel_call_arg(arg) }}
  {% endfor -%}
  {{ kernel_call_const_args() }}
  {%- endfilter %}
);
{% endif %}
{%- endmacro -%}

{%- macro kernel_call_const_args() -%}
{%- for c in op2const -%}
{% if(c._is_scalar) %}*{% endif %}{{ c.name }}
{% endfor -%}
{%- endmacro -%}

{%- macro kernel_call_arg(arg) -%}
{% if(arg._is_direct) -%}
  {{ typecast("__global", arg.data._cl_type + "*", "__private") -}}
  ({{ arg.data.name }} + (i_1 + shared_memory_offset) * {{ arg.data.cdim }})
{%- elif(arg._is_mat) -%}
  {{ arg.data.name }}_entry
{%- elif(arg._uses_itspace) -%}
  {{ dat_vec_name(arg) }}[idx_0]
{%- elif(arg._is_vec_map) -%}
  {{ dat_vec_name(arg) }}
{%- elif(arg._is_global_reduction) -%}
  {{ global_reduc_local_name(arg) }}
{%- elif(arg._is_indirect_reduction) -%}
  {{ reduc_arg_local_name(arg) }}
{%- elif(arg._is_global) -%}
  {{ arg.data.name }}
{%- else -%}
  &{{ shared_indirection_mapping_memory_name(arg) }}[{{ mapping_array_name(arg) }}[i_1 + shared_memory_offset] * {{ arg.data.cdim }}]
{%- endif -%}
{%- endmacro -%}

{%- macro typecast(storage, type, qualifier) -%}
({{ storage }} {{ type }}{% if(not codegen.amd) %} {{ qualifier }}{% endif %})
{%- endmacro -%}

{%- macro shared_memory_reduc_zeroing(arg) -%}
for (i_1 = get_local_id(0); i_1 < {{ shared_indirection_mapping_size_name(arg) }} * {{ arg.data.cdim }}; i_1 += get_local_size(0)) {
  {{ shared_indirection_mapping_memory_name(arg) }}[i_1] = 0;
}
{%- endmacro -%}

{%- macro matrix_support() -%}
void matrix_atomic_add(__global double* dst, double value);
void matrix_atomic_add(__global double* dst, double value)
{
#if defined(cl_khr_int64_base_atomics)
  {{ union_decl() }}
  do
  {
    old.val = *dst;
    new.val = old.val + value;
  } while (atom_cmpxchg((volatile __global unsigned long int*) dst, old.dummy, new.dummy) != old.dummy);
#else
  *dst = *dst + value;
#endif
}

void matrix_atomic_set(__global double* dst, double value);
void matrix_atomic_set(__global double* dst, double value)
{
#if defined(cl_khr_int64_base_atomics)
  {{ union_decl() }}
  do
  {
    old.val = 0.0;
    new.val = value;
  } while (atom_cmpxchg((volatile __global unsigned long int*) dst, old.dummy, new.dummy) != old.dummy);
#else
  *dst = value;
#endif
}

int rc2offset(__global int* mat_rowptr, __global int* mat_colidx, int r, int c);
int rc2offset(__global int* mat_rowptr, __global int* mat_colidx, int r, int c)
{
  int offset = mat_rowptr[r];
  int end = mat_rowptr[r+1];
  __global int * cursor;
  for (cursor = &mat_colidx[offset]; cursor < &mat_colidx[end]; ++cursor)
  {
    if (*cursor == c) break;
    ++offset;
  }
  return offset;
}

void matrix_add(__global double* mat_array, __global int* mat_rowptr, __global int* mat_colidx, int r, int c, double v);
void matrix_add(__global double* mat_array, __global int* mat_rowptr, __global int* mat_colidx, int r, int c, double v)
{
  int offset = rc2offset(mat_rowptr, mat_colidx, r, c);
  matrix_atomic_add(mat_array + offset, v);
}

void matrix_set(__global double* mat_array, __global int* mat_rowptr, __global int* mat_colidx, int r, int c, double v);
void matrix_set(__global double* mat_array, __global int* mat_rowptr, __global int* mat_colidx, int r, int c, double v)
{
  int offset = rc2offset(mat_rowptr, mat_colidx, r, c);
  matrix_atomic_set(mat_array + offset, v);
}
{%- endmacro -%}

{%- macro union_decl() -%}
  union {
    unsigned long dummy;
    double val;
  } new;

  union {
    unsigned long dummy;
    double val;
  } old;
{%- endmacro -%}



{{- header() }}

{% for arg in parloop._global_reduction_args -%}
  {{ reduction_kernel(arg) }}
{% endfor %}
{% if(parloop._matrix_args) %}
// Matrix support code
{{ matrix_support() }}
{% endif %}
{{ user_kernel }}
{{ kernel_stub() }}

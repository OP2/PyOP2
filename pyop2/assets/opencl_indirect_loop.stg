group opencl_indirect;

indirect_loop(parloop,const,op2const)::=<<
$header()$
$parloop._global_reduction_args:{$reduction_kernel()$};separator="\n"$
$parloop._kernel._inst_code$
$matrix_support()$
$kernel_stub()$
>>

kernel_stub()::=<<
__kernel
__attribute__((reqd_work_group_size($const.threads_per_block$, 1, 1)))
void $parloop._kernel._name$_stub(
  $parloop._unique_dats:{__global $it._cl_type$* $it._name$,};separator="\n"$
  $parloop._global_non_reduction_args:{__global $it._dat._cl_type$* $it._dat._name$,};separator="\n"$
  $parloop._dat_map_pairs:{__global int* $shared_indirection_mapping_arg_name()$,};separator="\n"$
  $parloop._args:{$if(it._is_indirect)$__global short* $mappingarrayname()$,$endif$};separator="\n"$
  $parloop._global_reduction_args:{__global $it._dat._cl_type$* $global_reduc_device_array_name()$,};separator="\n"$
  $parloop._unique_matrix:{__global $it._cl_type$* $it._name$, __global int* $it._name$_rowptr, __global int* $it._name$_colidx,};separator="\n"$
  $parloop._matrix_entry_maps:{__global int* $it._name$,};separator="\n"$
  $op2const:{__constant $it._cl_type$* $it._name$,};separator="\n"$
  __global int* p_ind_sizes,
  __global int* p_ind_offsets,
  __global int* p_blk_map,
  __global int* p_offset,
  __global int* p_nelems,
  __global int* p_nthrcol,
  __global int* p_thrcol,
  __private int block_offset
)
{
  __local char shared [$const.dynamic_shared_memory_size$] __attribute__((aligned(sizeof(long))));
  __local int shared_memory_offset;
  __local int active_threads_count;

  int nbytes;
  int block_id;

  int i_1;

$if(parloop._indirect_reduc_args)$
  __local int colors_count;
  __local int active_threads_count_ceiling;
  int color_1;
  int color_2;
  int i_2;
  // reduction args
  $parloop._indirect_reduc_args:{$it._dat._cl_type$ $reduc_arg_local_name()$[$it._dat._dim$];};separator="\n"$
$endif$

$if(parloop._global_reduction_args)$
  // global reduction local declarations
  $parloop._global_reduction_args:{$it._dat._cl_type$ $global_reduc_local_name()$[$it._dat._dim$];};separator="\n"$
$endif$

$if(parloop._matrix_args)$
  // local matrix entry
  $parloop._matrix_args:{__private $it._dat._cl_type$ $it._dat._name$_entry;};separator="\n"$
$endif$

  // shared indirection mappings
  $parloop._dat_map_pairs:{__global int* __local $shared_indirection_mapping_name()$;};separator="\n"$
  $parloop._dat_map_pairs:{__local int $shared_indirection_mapping_size_name()$;};separator="\n"$
  $parloop._dat_map_pairs:{__local $it._dat._cl_type$* __local $shared_indirection_mapping_memory_name()$;};separator="\n"$
  $parloop._dat_map_pairs:{const int $shared_indirection_mapping_idx_name()$ = $i0$;};separator="\n"$

  $parloop._nonreduc_vec_dat_map_pairs:{__local $it._dat._cl_type$* $dat_vec_name()$[$it._map._dim$];};separator="\n"$
  $parloop._reduc_vec_dat_map_pairs:{$it._dat._cl_type$* $dat_vec_name()$[$it._map._dim$];};separator="\n"$

  if (get_local_id(0) == 0)
  {
    block_id = p_blk_map[get_group_id(0) + block_offset];
    active_threads_count = p_nelems[block_id];
$if(parloop._indirect_reduc_args)$
    active_threads_count_ceiling = get_local_size(0) * (1 + (active_threads_count - 1) / get_local_size(0));
    colors_count = p_nthrcol[block_id];$endif$
    shared_memory_offset = p_offset[block_id];

    $parloop._dat_map_pairs:{$shared_indirection_mapping_size_name()$ = p_ind_sizes[$shared_indirection_mapping_idx_name()$ + block_id * $const.ninds$];};separator="\n"$

    $parloop._dat_map_pairs:{$shared_indirection_mapping_name()$ = $shared_indirection_mapping_arg_name()$ + p_ind_offsets[$shared_indirection_mapping_idx_name()$ + block_id * $const.ninds$];};separator="\n"$

    nbytes = 0;
    $parloop._dat_map_pairs:{$shared_indirection_mapping_memory_name()$ = (__local $it._dat._cl_type$*) (&shared[nbytes]);
nbytes += ROUND_UP($shared_indirection_mapping_size_name()$ * $it._dat._dim$ * sizeof($it._dat._cl_type$));};separator="\n"$
  }
  barrier(CLK_LOCAL_MEM_FENCE);

$if(parloop._read_dat_map_pairs)$
  // staging in of indirect dats
  $parloop._read_dat_map_pairs:stagingin();separator="\n"$
  barrier(CLK_LOCAL_MEM_FENCE);
$endif$

$if(parloop._indirect_reduc_dat_map_pairs)$
  // zeroing local memory for indirect reduction
  $parloop._indirect_reduc_dat_map_pairs:shared_memory_reduc_zeroing();separator="\n"$
  barrier(CLK_LOCAL_MEM_FENCE);
$endif$

$if(parloop._global_reduction_args)$
  // zeroing private memory for global reduction
  $parloop._global_reduction_args:{$global_reduction_local_zeroing()$};separator="\n"$
$endif$

$if(parloop._indirect_reduc_args)$
  for (i_1 = get_local_id(0); i_1 < active_threads_count_ceiling; i_1 += get_local_size(0)) {
    color_2 = -1;
    if (i_1 < active_threads_count)
    {
      $parloop._indirect_reduc_args:{$staged_arg_local_variable_zeroing()$};separator="\n"$

      $kernel_call()$
      color_2 = p_thrcol[i_1 + shared_memory_offset];
    }
    for (color_1 = 0; color_1 < colors_count; ++color_1)
    {
      // should there be a if + barrier pattern for each indirect reduction argument ?
      if (color_2 == color_1)
      {
        $parloop._indirect_reduc_args:{$reduction()$};separator="\n"$
      }
      barrier(CLK_LOCAL_MEM_FENCE);
    }
  }
$else$
  for (i_1 = get_local_id(0); i_1 < active_threads_count; i_1 += get_local_size(0))
  {
    $kernel_call()$
  }
$endif$

$if(parloop._indirect_reduc_dat_map_pairs)$
  $parloop._indirect_reduc_dat_map_pairs:{$reduction2()$};separator="\n"$
$endif$
$if(parloop._written_dat_map_pairs)$
  // staging out indirect dats
  barrier(CLK_LOCAL_MEM_FENCE);
  $parloop._written_dat_map_pairs:{$stagingout()$};separator="\n"$
$endif$
$if(parloop._global_reduction_args)$
  barrier(CLK_LOCAL_MEM_FENCE);
  // on device global reductions
  $parloop._global_reduction_args:{$on_device_global_reduction()$};separator="\n"$
$endif$
}
>>

shared_memory_reduc_zeroing()::=<<
for (i_1 = get_local_id(0); i_1 < $shared_indirection_mapping_size_name()$ * $it._dat._dim$; i_1 += get_local_size(0))
{
  $shared_indirection_mapping_memory_name()$[i_1] = 0;
}
>>

kernel_call()::=<<
$parloop._actual_args:{$if(it._is_vec_map)$$populate_vec_map()$$endif$};separator="\n"$
$if(parloop._it_space)$
$matrix_kernel_call()$
$else$
$parloop._kernel._name$(
  $parloop._actual_args:{$kernel_call_arg()$};separator=",\n"$
  $kernel_call_const_args()$
);
$endif$
>>

//rewrite: do recursive template
matrix_kernel_call()::=<<
// IterationSpace index loops ($parloop._it_space._extent_ranges:{$it$};separator=", "$)
$parloop._it_space._extent_ranges:{for (int idx_$i0$ = 0; idx_$i0$ < $it$; ++idx_$i0$) \{ }$
$parloop._matrix_args:{$it._dat._name$_entry = $it._dat._cl_type_zero$;};separator="\n"$
$parloop._kernel._name$(
  $parloop._actual_args:{$kernel_call_arg()$};separator=",\n"$
  $kernel_call_const_args()$
  $parloop._it_space._extent_ranges:{, idx_$i0$}$
);
$parloop._matrix_args:{arg|$if(arg._is_INC)$matrix_add$else$matrix_set$endif$(
  $arg._dat._name$,
  $arg._dat._name$_rowptr,
  $arg._dat._name$_colidx,
  $arg._map,parloop._it_space._extent_ranges:{map,ext|$map._name$[(i_1 + shared_memory_offset) * $ext$ + idx_$i0$],};separator="\n"$
  $arg._dat._name$_entry
);};separator="\n"$
$parloop._it_space._extent_ranges:{ \} }$
>>

kernel_call_const_args()::=<<$if(op2const)$$op2const:{c |, $if(c._is_scalar)$*$c._name$$else$$c._name$$endif$}$$endif$>>

kernel_call_arg()::=<<$if(it._is_direct)$$typecast(storage="__global",type={$it._dat._cl_type$*},qualifier="__private")$ ($it._dat._name$ + (i_1 + shared_memory_offset) * $it._dat._dim$)$elseif(it._is_mat)$&$it._dat._name$_entry$elseif(it._is_vec_map)$$dat_vec_name()$$elseif(it._is_global_reduction)$$global_reduc_local_name()$$elseif(it._is_indirect_reduction)$$reduc_arg_local_name()$$elseif(it._is_global)$$it._dat._name$$else$&$shared_indirection_mapping_memory_name()$[$mappingarrayname()$[i_1 + shared_memory_offset] * $it._dat._dim$]$endif$>>

typecast(storage,type,qualifier)::=<<($storage$ $type$$if(!const.amd)$ $qualifier$$endif$)>>

populate_vec_map()::=<<
// populate vec map
$if(it._is_indirect_reduction)$
$it._i_gen_vec:{$dat_vec_name()$[$it._idx$] = $reduc_arg_local_name()$;};separator="\n"$
$else$
$it._i_gen_vec:{$dat_vec_name()$[$it._idx$] = &$shared_indirection_mapping_memory_name()$[$mappingarrayname()$[i_1 + shared_memory_offset] * $it._dat._dim$];};separator="\n"$
$endif$
>>

staged_arg_local_variable_zeroing()::=<<
for (i_2 = 0; i_2 < $it._dat._dim$; ++i_2)
{
  $reduc_arg_local_name()$[i_2] = $it._dat._cl_type_zero$;
}
>>

reduction()::=<<
for (i_2 = 0; i_2 < $it._dat._dim$; ++i_2)
{
  $if(it._is_INC)$
  $shared_indirection_mapping_memory_name()$[i_2 + $mappingarrayname()$[i_1 + shared_memory_offset] * $it._dat._dim$] += $reduc_arg_local_name()$[i_2];
  $elseif(it._is_MIN)$
  $shared_indirection_mapping_memory_name()$[i_2 + $mappingarrayname()$[i_1 + shared_memory_offset] * $it._dat._dim$] = min($shared_indirection_mapping_memory_name()$[i_2 + $mappingarrayname()$[i_1 + shared_memory_offset] * $it._dat._dim$], $reduc_arg_local_name()$[i_2]);
  $elseif(it._is_MAX)$
  $shared_indirection_mapping_memory_name()$[i_2 + $mappingarrayname()$[i_1 + shared_memory_offset] * $it._dat._dim$] = max($shared_indirection_mapping_memory_name()$[i_2 + $mappingarrayname()$[i_1 + shared_memory_offset] * $it._dat._dim$], $reduc_arg_local_name()$[i_2]);
  $else$
  SOMETHING WENT SOUTH
  $endif$
}
>>

reduction2()::=<<
for (i_1 = get_local_id(0); i_1 < $shared_indirection_mapping_size_name()$ * $it._dat._dim$; i_1 += get_local_size(0))
{
  $it._dat._name$[i_1 % $it._dat._dim$ + $shared_indirection_mapping_name()$[i_1 / $it._dat._dim$] * $it._dat._dim$] += $shared_indirection_mapping_memory_name()$[i_1];
}
>>

stagingin()::=<<
for (i_1 = get_local_id(0); i_1 < $shared_indirection_mapping_size_name()$ * $it._dat._dim$; i_1 += get_local_size(0))
{
  $shared_indirection_mapping_memory_name()$[i_1] = $dat_arg_name()$[i_1 % $it._dat._dim$ + $shared_indirection_mapping_name()$[i_1 / $it._dat._dim$] * $it._dat._dim$];
}
>>

stagingout()::=<<
for (i_1 = get_local_id(0); i_1 < $shared_indirection_mapping_size_name()$ * $it._dat._dim$; i_1 += get_local_size(0))
{
  $it._dat._name$[i_1 % $it._dat._dim$ + $shared_indirection_mapping_name()$[i_1 / $it._dat._dim$] * $it._dat._dim$] = $shared_indirection_mapping_memory_name()$[i_1];
}
>>

global_reduction_local_zeroing()::=<<
for (i_1 = 0; i_1 < $it._dat._dim$; ++i_1)
{
  $global_reduc_local_name()$[i_1] = $it._dat._cl_type_zero$;
}
>>

on_device_global_reduction()::=<<
// THIS TEMPLATE SHOULD BE FACTORISED WITH DIRECT LOOPS REDUCTIONS
for (i_1 = 0; i_1 < $it._dat._dim$; ++i_1)
{
  $it._dat._name$_reduction_kernel(&$global_reduc_device_array_name()$[i_1 + get_group_id(0) * $it._dat._dim$], $global_reduc_local_name()$[i_1], (__local $it._dat._cl_type$*) shared);
}
>>

mappingarrayname()::=<<mapping_array_$it._dat._name$_at_$it._idx$_via$it._map._name$>>

global_reduc_local_name()::=<<$it._dat._name$_gbl_reduc_local>>
global_reduc_device_array_name()::=<<$it._dat._name$_gbl_reduc_device_array>>
dat_vec_name()::=<<$it._dat._name$_via_$it._map._name$_vec>>
reduc_arg_local_name()::=<<$it._dat._name$_via_$it._map._name$_at_$it._idx$_local>>
dat_arg_name()::=<<$it._dat._name$>>
shared_indirection_mapping_name()::=<<$it._dat._name$_via_$it._map._name$_indirection_map>>
shared_indirection_mapping_size_name()::=<<$it._dat._name$_via_$it._map._name$_indirection_size>>
shared_indirection_mapping_memory_name()::=<<$it._dat._name$_via_$it._map._name$_indirection>>
shared_indirection_mapping_idx_name()::=<<$it._dat._name$_via_$it._map._name$_idx>>
shared_indirection_mapping_arg_name()::=<<ind_$it._dat._name$_via_$it._map._name$_map>>

header()::=<<
/* Launch configuration:
 *   work group count     : $const.block_count$
 *   work group size      : $const.threads_per_block$
 *   partition size       : $const.partition_size$
 *   local memory size    : $const.dynamic_shared_memory_size$
 *   shared memory offset : $const.shared_memory_offset$
 *   warpsize             : $const.warpsize$
 */
#if defined(cl_khr_int64_base_atomics)
#pragma OPENCL EXTENSION cl_khr_int64_base_atomics : enable
#endif
#if defined(cl_khr_fp64)
#if defined(cl_amd_fp64)
#pragma OPENCL EXTENSION cl_amd_fp64 : enable
#else
#pragma OPENCL EXTENSION cl_khr_fp64 : enable
#endif
#elif defined(cl_amd_fp64)
#pragma OPENCL EXTENSION cl_amd_fp64 : enable
#endif

#define ROUND_UP(bytes) (((bytes) + 15) & ~15)
#define OP_WARPSIZE $const.warpsize$
#define OP2_STRIDE(arr, idx) (arr[idx])
>>

reduction_kernel()::=<<
__kernel
void $it._dat._name$_reduction_kernel (
  __global $it._dat._cl_type$* reduction_result,
  __private $it._dat._cl_type$ input_value,
  __local $it._dat._cl_type$* reduction_tmp_array
)
{
  barrier(CLK_LOCAL_MEM_FENCE);
  int lid = get_local_id(0);
  reduction_tmp_array[lid] = input_value;
  barrier(CLK_LOCAL_MEM_FENCE);

  for(int offset = 1;
      offset < (int) get_local_size(0);
      offset <<= 1)
  {
    int mask = (offset << 1) - 1;
    if ( ((lid & mask) == 0) && ((lid + offset) < (int) get_local_size(0)) ) {
      $reduction_op()$
    }
    barrier(CLK_LOCAL_MEM_FENCE);
  }
  if (lid == 0)
  {
    *reduction_result = reduction_tmp_array[0];
  }
}
>>

reduction_op()::=<<$if(it._is_INC)$
reduction_tmp_array[lid] += reduction_tmp_array[lid + offset];
$elseif(it._is_MIN)$
reduction_tmp_array[lid] += min(reduction_tmp_array[lid], reduction_tmp_array[lid + offset]);
$elseif(it._is_MAX)$
reduction_tmp_array[lid] += max(reduction_tmp_array[lid], reduction_tmp_array[lid + offset]);
$else$
SOMETHING WENT SOUTH;
$endif$>>

union_decl()::=<<
union
{
  unsigned long dummy;
  double val;
} new;
union
{
  unsigned long dummy;
  double val;
} old;
>>
matrix_support()::=<<
void matrix_atomic_add(__global double* dst, double value);
void matrix_atomic_add(__global double* dst, double value)
{
#if defined(cl_khr_int64_base_atomics)
  $union_decl()$
  do
  {
    old.val = *dst;
    new.val = old.val + value;
  } while (atom_cmpxchg((volatile __global unsigned long int*) dst, old.dummy, new.dummy) != old.dummy);
#else
  *dst = *dst + value;
#endif
}

void matrix_atomic_set(__global double* dst, double value);
void matrix_atomic_set(__global double* dst, double value)
{
#if defined(cl_khr_int64_base_atomics)
  $union_decl()$
  do
  {
    old.val = 0.0;
    new.val = value;
  } while (atom_cmpxchg((volatile __global unsigned long int*) dst, old.dummy, new.dummy) != old.dummy);
#else
  *dst = value;
#endif
}

int rc2offset(__global int* mat_rowptr, __global int* mat_colidx, int r, int c);
int rc2offset(__global int* mat_rowptr, __global int* mat_colidx, int r, int c)
{
  int offset = mat_rowptr[r];
  int end = mat_rowptr[r+1];
  __global int * cursor;
  for (cursor = &mat_colidx[offset]; cursor < &mat_colidx[end]; ++cursor)
  {
    if (*cursor == c) break;
    ++offset;
  }
  return offset;
}

void matrix_add(__global double* mat_array, __global int* mat_rowptr, __global int* mat_colidx, int r, int c, double v);
void matrix_add(__global double* mat_array, __global int* mat_rowptr, __global int* mat_colidx, int r, int c, double v)
{
  int offset = rc2offset(mat_rowptr, mat_colidx, r, c);
  matrix_atomic_add(mat_array + offset, v);
}

void matrix_set(__global double* mat_array, __global int* mat_rowptr, __global int* mat_colidx, int r, int c, double v);
void matrix_set(__global double* mat_array, __global int* mat_rowptr, __global int* mat_colidx, int r, int c, double v)
{
  int offset = rc2offset(mat_rowptr, mat_colidx, r, c);
  matrix_atomic_set(mat_array + offset, v);
}
>>
